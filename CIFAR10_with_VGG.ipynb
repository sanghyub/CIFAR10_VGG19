{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = \"\"\n",
    "img_size = 32\n",
    "num_channels = 3\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "num_classes = 10\n",
    "_num_files_train = 5\n",
    "_images_per_file = 10000\n",
    "_num_images_train = _num_files_train * _images_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def _unpickle(filename):  \n",
    "    \"\"\"\n",
    "    주어진 파일을 unpickle하고 data를 return 함\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(data_path, \"cifar-10-batches-py/\", filename)\n",
    "\n",
    "    print(\"Loading data: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file, encoding='bytes')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoded(class_numbers, num_classes=None):\n",
    "    \"\"\"\n",
    "    class_label들을 one_hot_encode로 생성해줌\n",
    "    예를 들어, class_number=2 그리고 num_classes=4 면\n",
    "    one-hot eoncoded label 은 [0, 0, 1, 0]\n",
    "    \n",
    "    :param class_numbers:\n",
    "        class_numbers의 integer 배열\n",
    "    :param_classes:\n",
    "        classes의 수\n",
    "    :return\n",
    "        2차원 shape [len(cls), num_classes]\n",
    "    \"\"\"\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(class_numbers) - 1\n",
    "\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "\n",
    "def _convert_images(raw):\n",
    "    \"\"\"\n",
    "    CIFAR-10 format으로 부터 image를 convert 하고\n",
    "    4차원 array로 return [image_number, height, width, channel]\n",
    "    pixels 0.0과 1.0사이에 floats\n",
    "    \"\"\"\n",
    "    raw_float = np.array(raw, dtype=float) / 255.0\n",
    "    # Reshape the array to 4-dimensions.\n",
    "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "def _load_data(filename): \n",
    "    \"\"\"\n",
    "    pickled data file을 load함\n",
    "    그리고 converted image와 각 image의 class-number를 return함\n",
    "    \"\"\"\n",
    "    data = _unpickle(filename)\n",
    "    raw_images = data[b'data']\n",
    "    cls = np.array(data[b'labels'])\n",
    "    images = _convert_images(raw_images)\n",
    "    return images, cls\n",
    "\n",
    "def load_class_names():\n",
    "    \"\"\"\n",
    "    cifar10 data set에 class들에 names를 load함\n",
    "    이름 list를 return 함\n",
    "    예를 들면 name[3] 은 class-number 3\n",
    "    \"\"\"\n",
    "    raw = _unpickle(filename=\"batches.meta\")[b'label_names']\n",
    "    names = [x.decode('utf-8') for x in raw]\n",
    "    return names\n",
    "\n",
    "\n",
    "def load_training_data(): \n",
    "    \"\"\"\n",
    "    cifar10의 모든 training data를 load함\n",
    "    data set은 5개의 파일로 나눠져 있고 여기서 merge함\n",
    "    image, class -number, one-hot encode된 class-label을 return 함\n",
    "    \"\"\"\n",
    "    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n",
    "    cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
    "\n",
    "    begin = 0\n",
    "    for i in range(_num_files_train):\n",
    "        images_batch, cls_batch = _load_data(filename=\"data_batch_\" + str(i + 1))\n",
    "        \n",
    "        num_images = len(images_batch)\n",
    "        end = begin + num_images\n",
    "        images[begin:end, :] = images_batch\n",
    "        cls[begin:end] = cls_batch\n",
    "        begin = end\n",
    "\n",
    "    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    모든 test-data set을 load함\n",
    "    image , class-numbers and one-hot encode된 class-labels를 return 한다.\n",
    "    \"\"\"\n",
    "    images, cls = _load_data(filename=\"test_batch\")\n",
    "    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(n_inputs, n_outputs, uniform=True):  \n",
    "    \"\"\"\n",
    "    param n_inputs:\n",
    "        input nodes의 수\n",
    "    param n_outputs:\n",
    "        output nodes의 수\n",
    "    param uniform:\n",
    "        true라면 uniform distribution을 사용 , 아니면 normal을 사용\n",
    "    \"\"\"\n",
    "    \n",
    "    if uniform:\n",
    "        init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)\n",
    "    \n",
    "    \n",
    "def variable_xavier(name,shape):\n",
    "    var = tf.get_variable(name = name, shape = shape , initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return var\n",
    "    \n",
    "def restoreSaver(session,check_dir):\n",
    "    \"\"\"\n",
    "    (*) 실행하기 전 last_epoch = tf.Variable(0, name='last_epoch') 선언\n",
    "    (*) session.run(global_variables_initializer()) 하기 전에 선언시켜줘야함.\n",
    "    (*) 복귀된 epoch는 session.run(last_epoch)의 리턴값으로 얻어짐.\n",
    "        Restore Saver\n",
    "    :param session:\n",
    "        현재 실행시킨 session\n",
    "    :param check_dir:\n",
    "        저장된 디렉토리 경로\n",
    "    :return:\n",
    "        세이버 리턴\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "    check_point = tf.train.get_checkpoint_state(check_dir)\n",
    "\n",
    "    if check_point and check_point.model_checkpoint_path:\n",
    "        try:\n",
    "            saver.restore(session, check_point.model_checkpoint_path)\n",
    "            print('successfully loaded : ', check_point.model_checkpoint_path)\n",
    "        except:\n",
    "            print('fail to load')\n",
    "    else:\n",
    "        print('could not find load data')\n",
    "\n",
    "    return saver\n",
    "\n",
    "def saveSaver(session, last_epoch ,global_step ,saver, check_dir, model_name = 'model'):\n",
    "    \"\"\"\n",
    "        체크포인트 세이브\n",
    "    :param session:\n",
    "        현재 세션\n",
    "    :param last_epoch:\n",
    "        마지막 epoch\n",
    "        last_epoch - tf.Variable\n",
    "    :param global_step:\n",
    "        현재 global_step\n",
    "    :param saver:\n",
    "        저장시킬 saver\n",
    "    :param check_dir:\n",
    "        저장시킬 directory - restoreSaver 의 check_dir 과 같음.\n",
    "    :param model_name:\n",
    "        저장시킬 모델이름.\n",
    "        default = 'model'\n",
    "    :return:\n",
    "        세이버 리턴\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(check_dir):\n",
    "        os.makedirs(check_dir)\n",
    "    try:\n",
    "        session.run(last_epoch.assign(global_step+1))\n",
    "        saver.save(sess=session,save_path = check_dir+'/'+model_name, global_step = global_step)\n",
    "        print('step',global_step,'successfully save dir :', check_dir+'/'+model_name+'-'+str(global_step))\n",
    "    except:\n",
    "        print('fail to save')\n",
    "\n",
    "    return saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class batch_norm(object): \n",
    "    \"\"\"\n",
    "    batch_normalization class\n",
    "    epsilon은 1e-5\n",
    "    momentum 0.9로 초기화 하고 씀\n",
    "    __call__함수를 정의해서 사용할 때 변수에 call 해서 사용할 수 있음\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\")\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        \n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                        decay=self.momentum,\n",
    "                                        updates_collections=None,\n",
    "                                        epsilon=self.epsilon,\n",
    "                                        scale=True,\n",
    "                                        is_training=train,\n",
    "                                        scope=self.name)\n",
    "\n",
    "\n",
    "def conv2d(t_input,ksize,strides,padding,layerName,initializer=variable_xavier):\n",
    "    \"\"\"\n",
    "    convolution function\n",
    "    param t_input:\n",
    "        convolution하기 위해 들어오는 input\n",
    "    param ksize:\n",
    "        filter size [x, x, x ,x]\n",
    "    param strides:\n",
    "        strides [1,x,x,1]\n",
    "    param layerName:\n",
    "        레이어 이름\n",
    "    param initializer:\n",
    "        default로 xavier를 사용해서 initialization\n",
    "    \"\"\"\n",
    "    W = initializer(name=layerName+\"/Weight\", shape=ksize)\n",
    "    output = tf.nn.conv2d(input = t_input, filter = W, strides = strides, padding = padding)\n",
    "    return output\n",
    "\n",
    "\n",
    "def maxPool(t_input,ksize,strides,padding,layerName):\n",
    "    \"\"\"\n",
    "     param t_input:\n",
    "        max_pooling 하기 위해 들어오는 input\n",
    "    param ksize:\n",
    "        filter size [x, x, x ,x]\n",
    "    param strides:\n",
    "        strides [1,x,x,1]\n",
    "    param layerName:\n",
    "        레이어 이름\n",
    "    \"\"\"\n",
    "    output = tf.nn.max_pool(t_input, ksize = ksize, strides=strides, padding = padding)\n",
    "    return output\n",
    "\n",
    "\n",
    "def flatten(t_input, flatDim, layerName='flattenLayer'):\n",
    "    \"\"\"\n",
    "    t_input을 [-1,flatDim] 모양으로 reshape 해줌\n",
    "    \"\"\"\n",
    "    flatten = tf.reshape(t_input, [-1, flatDim])\n",
    "    return flatten\n",
    "\n",
    "def fullyConnected(t_input, shape, layerName, initializer = variable_xavier): \n",
    "    \"\"\"\n",
    "    W와 B를 xavier를 이용해서 initializaion 해줌\n",
    "    fully connected로 만들어줌\n",
    "    \"\"\"\n",
    "    W = initializer(name=layerName+'/weight',shape = shape)\n",
    "    B = initializer(name=layerName+'/bias',shape = shape[-1])\n",
    "    preActivate = tf.matmul(t_input, W) + B\n",
    "    return preActivate\n",
    "\n",
    "def crossEntropy(labels, logits, name = 'lossFunction'):\n",
    "    \"\"\"\n",
    "    cross_entropy에 reduce_mean을 해서 return 해줌\n",
    "    \"\"\"\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits))\n",
    "    return cost\n",
    "\n",
    "def lrelu(x,leak=0.2):\n",
    "    \"\"\"\n",
    "    leakyRelu를 return 해줌\n",
    "    \"\"\"\n",
    "    return tf.maximum(x,leak*x)\n",
    "\n",
    "def trainOptimizer(cost, learning_rate = 1e-3, optimizer = tf.train.AdamOptimizer, name = 'train'):\n",
    "    \"\"\"\n",
    "    train할 optimizer를 정해줌\n",
    "    여기서는 learning late를 1e-3으로 하고\n",
    "    adamOptimizer를 사용함\n",
    "    다른 optimizer로 바꿀 수 있음\n",
    "    \"\"\"\n",
    "    optimize = optimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    return optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class VGG19:  #vgg19 layers\n",
    "    def __init__(self, input, t):\n",
    "        if input is None: return\n",
    "        self.out, self.phi = self.build_model(input)\n",
    "        self.loss = self.inference_loss(self.out, t)\n",
    "\n",
    "    def build_model(self, t_input, reuse=False):\n",
    "            phi = []\n",
    "            with tf.variable_scope('conv1a'):      #[None, 32, 32, 3]\n",
    "                conv1a = conv2d(\n",
    "                                layerName='vgg19_conv1a',\n",
    "                                t_input=t_input,        #처음에 들어가는 input\n",
    "                                ksize= [3, 3, 3, 64],   #filter size는 3x3,  InChennel =3, outChennel =64\n",
    "                                strides=[1,1,1,1],      #stride =1\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()      #Batch_norm을 BN으로 사용할 것임\n",
    "                conv1a = BN(conv1a)    #con1a에 bach_norm을 적용\n",
    "                conv1a = lrelu(conv1a)   #bach_norm에서 나온 값을 leakyRelu activation function을 사용함\n",
    "                print(\"conv1a :\", conv1a)  #[None, 32, 32, 64]\n",
    "\n",
    "\n",
    "            with tf.variable_scope('conv1b'):     \n",
    "                conv1b = conv2d(layerName='vgg19_conv1b',\n",
    "                                t_input=conv1a,\n",
    "                                ksize=[3,3,64,64],\n",
    "                                strides=[1,1,1,1],\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv1b =BN(conv1b)\n",
    "                conv1b =lrelu(conv1b)\n",
    "                print(\"conv1b :\", conv1b)     #[None, 32, 32, 64]\n",
    "            phi.append(conv1b)\n",
    "\n",
    "            pool_first = maxPool(layerName='pool_first', #첫번째 max_pooling\n",
    "                                 t_input=conv1b,            \n",
    "                                 ksize=[1,2,2,1],        #ksize= 2X2\n",
    "                                 strides=[1,2,2,1],      #stride = 2\n",
    "                                 padding='SAME')        \n",
    "            \n",
    "            print(\"max_pool_first :\",pool_first)       #[None, 16, 16, 128]\n",
    "            # pool 1\n",
    "\n",
    "            with tf.variable_scope('conv2a'):\n",
    "                conv2a = conv2d(layerName='vgg19_conv2a',\n",
    "                                t_input=pool_first,\n",
    "                                ksize=[3, 3, 64, 128],\n",
    "                                strides=[1,1,1,1],\n",
    "                                padding='SAME')\n",
    "\n",
    "                BN = batch_norm()\n",
    "                conv2a = BN(conv2a)\n",
    "                conv2a = lrelu(conv2a)\n",
    "                print(\"conv2a :\", conv2a)            #[None, 16, 16, 128]\n",
    "\n",
    "\n",
    "            with tf.variable_scope('conv2b'):\n",
    "                conv2b = conv2d(layerName='vgg19_conv2b',\n",
    "                                t_input=conv2a,\n",
    "                                ksize=[3, 3, 128, 128],\n",
    "                                strides=[1,1,1,1],\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv2b = BN(conv2b)\n",
    "                conv2b = lrelu(conv2b)\n",
    "                print(\"conv2b :\", conv2b)         #[None, 16, 16, 128]\n",
    "            phi.append(conv2b)\n",
    "\n",
    "            pool_second=maxPool(layerName='pool_first',\n",
    "                                t_input=conv2b,\n",
    "                                ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "            \n",
    "            print(\"max_pool_second :\",pool_second)    #[None, 8, 8, 128]\n",
    "            # pool 2\n",
    "\n",
    "            with tf.variable_scope('conv3a'):\n",
    "                conv3a = conv2d(layerName='vgg19_conv3a',\n",
    "                                t_input=pool_second,\n",
    "                                ksize=[3, 3, 128, 256],\n",
    "                                strides=[1,1,1,1],\n",
    "                                padding='SAME')\n",
    "\n",
    "                BN = batch_norm()\n",
    "                conv3a = BN(conv3a)\n",
    "                conv3a = lrelu(conv3a)\n",
    "                print(\"conv3a :\", conv3a)       #[None, 8, 8, 256]\n",
    "\n",
    "\n",
    "            with tf.variable_scope('conv3b'):\n",
    "                conv3b =conv2d(layerName='vgg19_conv3b',\n",
    "                                       t_input=conv3a,\n",
    "                                       ksize=[3, 3, 256, 256],\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding='SAME')\n",
    "\n",
    "                BN = batch_norm()\n",
    "                conv3b = BN(conv3b)\n",
    "                conv3b = lrelu(conv3b)\n",
    "                print(\"conv3b :\", conv3b)\n",
    "\n",
    "            with tf.variable_scope('conv3c'):\n",
    "                conv3c = conv2d(layerName='vgg19_conv3c',\n",
    "                                       t_input=conv3b,\n",
    "                                       ksize=[3, 3, 256, 256],\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv3c = BN(conv3c)\n",
    "                conv3c = lrelu(conv3c)\n",
    "                print(\"conv3c :\", conv3c)      #[None, 8, 8, 256]\n",
    "\n",
    "            with tf.variable_scope('conv3d'):\n",
    "                conv3d = conv2d(layerName='vgg19_conv3d',\n",
    "                                       t_input=conv3c,\n",
    "                                       ksize=[3, 3, 256, 256],\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv3d = BN(conv3d)\n",
    "                conv3d = lrelu(conv3d)\n",
    "                print(\"conv3d :\", conv3d)       #[None, 8, 8, 256]\n",
    "            phi.append(conv3d)\n",
    "\n",
    "            pool_third =maxPool(layerName='pool_first',\n",
    "                               t_input=conv3d,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1],\n",
    "                                padding='SAME')\n",
    "            \n",
    "            print(\"max_pool_third :\",pool_third)  #[None, 4, 4, 256]\n",
    "\n",
    "            with tf.variable_scope('conv4a'):\n",
    "                conv4a = conv2d(layerName='vgg19_conv4a',\n",
    "                                       t_input=pool_third,\n",
    "                                       ksize=[3, 3, 256, 512],\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv4a = BN(conv4a)\n",
    "                conv4a = lrelu(conv4a)\n",
    "                print(\"conv4a :\", conv4a)      #[None, 4, 4, 512]\n",
    "\n",
    "            with tf.variable_scope('conv4b'):\n",
    "                conv4b = conv2d(layerName='vgg19_conv4b',\n",
    "                                       t_input=conv4a,\n",
    "                                       ksize=[3, 3, 512, 512],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv4b = BN(conv4b)\n",
    "                conv4b = lrelu(conv4b)\n",
    "                print(\"conv4b :\", conv4b)    #[None, 4, 4, 512]\n",
    "\n",
    "\n",
    "            with tf.variable_scope('conv4c'):\n",
    "                conv4c = conv2d(layerName='vgg19_conv4c',\n",
    "                                       t_input=conv4b,\n",
    "                                       ksize=[3, 3, 512, 512],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv4c = BN(conv4c)\n",
    "                conv4c = lrelu(conv4c)\n",
    "                print(\"conv4c :\", conv4c)    #[None, 4, 4, 512]\n",
    "\n",
    "\n",
    "            with tf.variable_scope('conv4d'):\n",
    "                conv4d = conv2d(layerName='vgg19_conv4d',\n",
    "                                       t_input=conv4c,\n",
    "                                       ksize=[3, 3, 512, 512],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv4d = BN(conv4d)\n",
    "                conv4d = lrelu(conv4d)\n",
    "                print(\"conv4d :\", conv4d)     #[None, 4, 4, 512]\n",
    "\n",
    "            phi.append(conv4d)\n",
    "\n",
    "            pool_fourth = maxPool(layerName='pool_first',\n",
    "                                  t_input=conv4d,\n",
    "                                  ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "            print(\"max_pool_fourth :\",pool_fourth)     #[None, 2, 2, 512]\n",
    "\n",
    "            with tf.variable_scope('conv5a'):\n",
    "                conv5a = conv2d(layerName='vgg19_conv5a',\n",
    "                                       t_input=pool_fourth,\n",
    "                                       ksize=[3, 3, 512, 512],\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv5a = BN(conv5a)\n",
    "                conv5a = lrelu(conv5a)\n",
    "                print(\"conv5a :\", conv5a)   #[None, 2, 2, 512]\n",
    "\n",
    "            with tf.variable_scope('conv5b'):\n",
    "                conv5b = conv2d(layerName='vgg19_conv5b',\n",
    "                                t_input=conv5a,\n",
    "                                 ksize=[3, 3, 512, 512],\n",
    "                                strides=[1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv5b = BN(conv5b)\n",
    "                conv5b = lrelu(conv5b)\n",
    "                print(\"conv5b :\", conv5b)   #[None, 2, 2, 512]\n",
    "\n",
    "            with tf.variable_scope('conv5c'):\n",
    "                conv5c = conv2d(layerName='vgg19_conv5c',\n",
    "                                t_input=conv5b,\n",
    "                                ksize=[3, 3, 512, 512],\n",
    "                                strides=[1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv5c = BN(conv5c)\n",
    "                conv5c = lrelu(conv5c)\n",
    "                print(\"conv5c :\", conv5c)   #[None, 2, 2, 512]\n",
    "\n",
    "            with tf.variable_scope('conv5d'):\n",
    "                conv5d = conv2d(layerName='vgg19_conv5d',\n",
    "                                t_input=conv5c,\n",
    "                                ksize=[3, 3, 512, 512],\n",
    "                                strides=[1, 1, 1, 1],\n",
    "                                padding='SAME')\n",
    "                BN = batch_norm()\n",
    "                conv5d = BN(conv5d)\n",
    "                conv5d = lrelu(conv5d)\n",
    "                print(\"conv5d :\", conv5d)  #[None, 2, 2, 512]\n",
    "\n",
    "            phi.append(conv5d)\n",
    "\n",
    "            pool_fifth = maxPool(layerName='pool_first',\n",
    "                                 t_input=conv5d,\n",
    "                                 ksize=[1, 2, 2, 1],\n",
    "                                 strides=[1, 2, 2, 1],\n",
    "                                 padding='SAME')\n",
    "            print(\"max_pool_fifth :\",pool_fifth)   #[None, 1, 1, 512]\n",
    "\n",
    "            flat = flatten(t_input=pool_fifth, flatDim=512)      #fully_connected를 하기전에 flatten을 해줌\n",
    "            print(\"flatten :\", flat)                    #[None,512]\n",
    "\n",
    "            with tf.variable_scope('fc1'):\n",
    "                fc1 = fullyConnected(flat, [512, 4096], layerName='fc1')     #[None, 512]를 -> [None,4096]\n",
    "                fc1 = lrelu(fc1)\n",
    "                print(\"fc1 :\",fc1)\n",
    "                \n",
    "            with tf.variable_scope('fc2'):\n",
    "                fc2 = fullyConnected(fc1, [4096, 4096], layerName='fc2')     #[None ,4096] -> [None,4096]\n",
    "                fc2 = lrelu(fc2)\n",
    "                print(\"fc2 :\",fc2)\n",
    "                \n",
    "            with tf.variable_scope('softmax'):\n",
    "                fc3 = fullyConnected(fc2, [4096, 10], layerName='fc3')      #[None ,4096] -> [None,10]\n",
    "                print(\"fc3 :\",fc3)\n",
    "                \n",
    "            return fc3, phi\n",
    "\n",
    "\n",
    "    def inference_loss(self, out, t):     #loss function\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=t,logits=out)\n",
    "        return tf.reduce_mean(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: cifar-10-batches-py/batches.meta\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "conv1a : Tensor(\"conv1a/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "conv1b : Tensor(\"conv1b/Maximum:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "max_pool_first : Tensor(\"MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "conv2a : Tensor(\"conv2a/Maximum:0\", shape=(?, 16, 16, 128), dtype=float32)\n",
      "conv2b : Tensor(\"conv2b/Maximum:0\", shape=(?, 16, 16, 128), dtype=float32)\n",
      "max_pool_second : Tensor(\"MaxPool_1:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "conv3a : Tensor(\"conv3a/Maximum:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "conv3b : Tensor(\"conv3b/Maximum:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "conv3c : Tensor(\"conv3c/Maximum:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "conv3d : Tensor(\"conv3d/Maximum:0\", shape=(?, 8, 8, 256), dtype=float32)\n",
      "max_pool_third : Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      "conv4a : Tensor(\"conv4a/Maximum:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "conv4b : Tensor(\"conv4b/Maximum:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "conv4c : Tensor(\"conv4c/Maximum:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "conv4d : Tensor(\"conv4d/Maximum:0\", shape=(?, 4, 4, 512), dtype=float32)\n",
      "max_pool_fourth : Tensor(\"MaxPool_3:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "conv5a : Tensor(\"conv5a/Maximum:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "conv5b : Tensor(\"conv5b/Maximum:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "conv5c : Tensor(\"conv5c/Maximum:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "conv5d : Tensor(\"conv5d/Maximum:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "max_pool_fifth : Tensor(\"MaxPool_4:0\", shape=(?, 1, 1, 512), dtype=float32)\n",
      "flatten : Tensor(\"Reshape:0\", shape=(?, 512), dtype=float32)\n",
      "fc1 : Tensor(\"fc1/Maximum:0\", shape=(?, 4096), dtype=float32)\n",
      "fc2 : Tensor(\"fc2/Maximum:0\", shape=(?, 4096), dtype=float32)\n",
      "fc3 : Tensor(\"softmax/add:0\", shape=(?, 10), dtype=float32)\n",
      "successfully loaded :  ./checkpoint\\test_model-1\n",
      "Learinig Start....\n",
      "Loading data: cifar-10-batches-py/data_batch_1\n",
      "Loading data: cifar-10-batches-py/data_batch_2\n",
      "Loading data: cifar-10-batches-py/data_batch_3\n",
      "Loading data: cifar-10-batches-py/data_batch_4\n",
      "Loading data: cifar-10-batches-py/data_batch_5\n",
      "Loading data: cifar-10-batches-py/test_batch\n",
      "트레이닝 데이터 수 :  50000\n",
      "===EPOCH    2===\n",
      "[Test]Accuracy at epoch 2: 0.494\n",
      "[Training]Accuracy at epoch 2: 0.47\n",
      "step 2 successfully save dir : ./checkpoint/test_model-2\n",
      "===EPOCH    3===\n",
      "[Test]Accuracy at epoch 3: 0.602\n",
      "[Training]Accuracy at epoch 3: 0.58\n",
      "step 3 successfully save dir : ./checkpoint/test_model-3\n",
      "===EPOCH    4===\n",
      "[Test]Accuracy at epoch 4: 0.667\n",
      "[Training]Accuracy at epoch 4: 0.68\n",
      "step 4 successfully save dir : ./checkpoint/test_model-4\n",
      "===EPOCH    5===\n",
      "[Test]Accuracy at epoch 5: 0.706\n",
      "[Training]Accuracy at epoch 5: 0.74\n",
      "step 5 successfully save dir : ./checkpoint/test_model-5\n",
      "===EPOCH    6===\n",
      "[Test]Accuracy at epoch 6: 0.738\n",
      "[Training]Accuracy at epoch 6: 0.72\n",
      "step 6 successfully save dir : ./checkpoint/test_model-6\n",
      "===EPOCH    7===\n",
      "[Test]Accuracy at epoch 7: 0.744\n",
      "[Training]Accuracy at epoch 7: 0.77\n",
      "step 7 successfully save dir : ./checkpoint/test_model-7\n",
      "===EPOCH    8===\n",
      "[Test]Accuracy at epoch 8: 0.758\n",
      "[Training]Accuracy at epoch 8: 0.84\n",
      "step 8 successfully save dir : ./checkpoint/test_model-8\n",
      "===EPOCH    9===\n",
      "[Test]Accuracy at epoch 9: 0.745\n",
      "[Training]Accuracy at epoch 9: 0.83\n",
      "step 9 successfully save dir : ./checkpoint/test_model-9\n",
      "===EPOCH   10===\n",
      "[Test]Accuracy at epoch 10: 0.177\n",
      "[Training]Accuracy at epoch 10: 0.18\n",
      "step 10 successfully save dir : ./checkpoint/test_model-10\n",
      "===EPOCH   11===\n",
      "[Test]Accuracy at epoch 11: 0.335\n",
      "[Training]Accuracy at epoch 11: 0.34\n",
      "step 11 successfully save dir : ./checkpoint/test_model-11\n",
      "===EPOCH   12===\n",
      "[Test]Accuracy at epoch 12: 0.4\n",
      "[Training]Accuracy at epoch 12: 0.4\n",
      "step 12 successfully save dir : ./checkpoint/test_model-12\n",
      "===EPOCH   13===\n",
      "[Test]Accuracy at epoch 13: 0.522\n",
      "[Training]Accuracy at epoch 13: 0.41\n",
      "step 13 successfully save dir : ./checkpoint/test_model-13\n",
      "===EPOCH   14===\n",
      "[Test]Accuracy at epoch 14: 0.61\n",
      "[Training]Accuracy at epoch 14: 0.53\n",
      "step 14 successfully save dir : ./checkpoint/test_model-14\n",
      "===EPOCH   15===\n",
      "[Test]Accuracy at epoch 15: 0.694\n",
      "[Training]Accuracy at epoch 15: 0.65\n",
      "step 15 successfully save dir : ./checkpoint/test_model-15\n",
      "===EPOCH   16===\n",
      "[Test]Accuracy at epoch 16: 0.72\n",
      "[Training]Accuracy at epoch 16: 0.68\n",
      "step 16 successfully save dir : ./checkpoint/test_model-16\n",
      "===EPOCH   17===\n",
      "[Test]Accuracy at epoch 17: 0.732\n",
      "[Training]Accuracy at epoch 17: 0.76\n",
      "step 17 successfully save dir : ./checkpoint/test_model-17\n",
      "===EPOCH   18===\n",
      "[Test]Accuracy at epoch 18: 0.791\n",
      "[Training]Accuracy at epoch 18: 0.81\n",
      "step 18 successfully save dir : ./checkpoint/test_model-18\n",
      "===EPOCH   19===\n",
      "[Test]Accuracy at epoch 19: 0.787\n",
      "[Training]Accuracy at epoch 19: 0.86\n",
      "step 19 successfully save dir : ./checkpoint/test_model-19\n",
      "===EPOCH   20===\n",
      "[Test]Accuracy at epoch 20: 0.818\n",
      "[Training]Accuracy at epoch 20: 0.86\n",
      "step 20 successfully save dir : ./checkpoint/test_model-20\n",
      "===EPOCH   21===\n",
      "[Test]Accuracy at epoch 21: 0.792\n",
      "[Training]Accuracy at epoch 21: 0.9\n",
      "step 21 successfully save dir : ./checkpoint/test_model-21\n",
      "===EPOCH   22===\n",
      "[Test]Accuracy at epoch 22: 0.787\n",
      "[Training]Accuracy at epoch 22: 0.83\n",
      "step 22 successfully save dir : ./checkpoint/test_model-22\n",
      "===EPOCH   23===\n",
      "[Test]Accuracy at epoch 23: 0.833\n",
      "[Training]Accuracy at epoch 23: 0.93\n",
      "step 23 successfully save dir : ./checkpoint/test_model-23\n",
      "===EPOCH   24===\n",
      "[Test]Accuracy at epoch 24: 0.811\n",
      "[Training]Accuracy at epoch 24: 0.93\n",
      "step 24 successfully save dir : ./checkpoint/test_model-24\n",
      "===EPOCH   25===\n",
      "[Test]Accuracy at epoch 25: 0.817\n",
      "[Training]Accuracy at epoch 25: 0.94\n",
      "step 25 successfully save dir : ./checkpoint/test_model-25\n",
      "===EPOCH   26===\n",
      "[Test]Accuracy at epoch 26: 0.808\n",
      "[Training]Accuracy at epoch 26: 0.94\n",
      "step 26 successfully save dir : ./checkpoint/test_model-26\n",
      "===EPOCH   27===\n",
      "[Test]Accuracy at epoch 27: 0.837\n",
      "[Training]Accuracy at epoch 27: 0.98\n",
      "step 27 successfully save dir : ./checkpoint/test_model-27\n",
      "===EPOCH   28===\n",
      "[Test]Accuracy at epoch 28: 0.809\n",
      "[Training]Accuracy at epoch 28: 0.96\n",
      "step 28 successfully save dir : ./checkpoint/test_model-28\n",
      "===EPOCH   29===\n",
      "[Test]Accuracy at epoch 29: 0.813\n",
      "[Training]Accuracy at epoch 29: 0.98\n",
      "step 29 successfully save dir : ./checkpoint/test_model-29\n",
      "===EPOCH   30===\n",
      "[Test]Accuracy at epoch 30: 0.822\n",
      "[Training]Accuracy at epoch 30: 0.94\n",
      "step 30 successfully save dir : ./checkpoint/test_model-30\n",
      "===EPOCH   31===\n",
      "[Test]Accuracy at epoch 31: 0.815\n",
      "[Training]Accuracy at epoch 31: 0.94\n",
      "step 31 successfully save dir : ./checkpoint/test_model-31\n",
      "===EPOCH   32===\n",
      "[Test]Accuracy at epoch 32: 0.505\n",
      "[Training]Accuracy at epoch 32: 0.53\n",
      "step 32 successfully save dir : ./checkpoint/test_model-32\n",
      "===EPOCH   33===\n",
      "[Test]Accuracy at epoch 33: 0.667\n",
      "[Training]Accuracy at epoch 33: 0.74\n",
      "step 33 successfully save dir : ./checkpoint/test_model-33\n",
      "===EPOCH   34===\n",
      "[Test]Accuracy at epoch 34: 0.815\n",
      "[Training]Accuracy at epoch 34: 0.89\n",
      "step 34 successfully save dir : ./checkpoint/test_model-34\n",
      "===EPOCH   35===\n",
      "[Test]Accuracy at epoch 35: 0.834\n",
      "[Training]Accuracy at epoch 35: 0.95\n",
      "step 35 successfully save dir : ./checkpoint/test_model-35\n",
      "===EPOCH   36===\n",
      "[Test]Accuracy at epoch 36: 0.824\n",
      "[Training]Accuracy at epoch 36: 0.94\n",
      "step 36 successfully save dir : ./checkpoint/test_model-36\n",
      "===EPOCH   37===\n",
      "[Test]Accuracy at epoch 37: 0.813\n",
      "[Training]Accuracy at epoch 37: 0.98\n",
      "step 37 successfully save dir : ./checkpoint/test_model-37\n",
      "===EPOCH   38===\n",
      "[Test]Accuracy at epoch 38: 0.807\n",
      "[Training]Accuracy at epoch 38: 0.97\n",
      "step 38 successfully save dir : ./checkpoint/test_model-38\n",
      "===EPOCH   39===\n",
      "[Test]Accuracy at epoch 39: 0.83\n",
      "[Training]Accuracy at epoch 39: 0.94\n",
      "step 39 successfully save dir : ./checkpoint/test_model-39\n",
      "===EPOCH   40===\n",
      "[Test]Accuracy at epoch 40: 0.851\n",
      "[Training]Accuracy at epoch 40: 0.99\n",
      "step 40 successfully save dir : ./checkpoint/test_model-40\n",
      "===EPOCH   41===\n",
      "[Test]Accuracy at epoch 41: 0.843\n",
      "[Training]Accuracy at epoch 41: 1.0\n",
      "step 41 successfully save dir : ./checkpoint/test_model-41\n",
      "===EPOCH   42===\n",
      "[Test]Accuracy at epoch 42: 0.841\n",
      "[Training]Accuracy at epoch 42: 0.97\n",
      "step 42 successfully save dir : ./checkpoint/test_model-42\n",
      "===EPOCH   43===\n",
      "[Test]Accuracy at epoch 43: 0.84\n",
      "[Training]Accuracy at epoch 43: 0.99\n",
      "step 43 successfully save dir : ./checkpoint/test_model-43\n",
      "===EPOCH   44===\n",
      "[Test]Accuracy at epoch 44: 0.847\n",
      "[Training]Accuracy at epoch 44: 1.0\n",
      "step 44 successfully save dir : ./checkpoint/test_model-44\n",
      "===EPOCH   45===\n",
      "[Test]Accuracy at epoch 45: 0.836\n",
      "[Training]Accuracy at epoch 45: 0.98\n",
      "step 45 successfully save dir : ./checkpoint/test_model-45\n",
      "===EPOCH   46===\n",
      "[Test]Accuracy at epoch 46: 0.851\n",
      "[Training]Accuracy at epoch 46: 0.98\n",
      "step 46 successfully save dir : ./checkpoint/test_model-46\n",
      "===EPOCH   47===\n",
      "[Test]Accuracy at epoch 47: 0.843\n",
      "[Training]Accuracy at epoch 47: 0.99\n",
      "step 47 successfully save dir : ./checkpoint/test_model-47\n",
      "===EPOCH   48===\n",
      "[Test]Accuracy at epoch 48: 0.846\n",
      "[Training]Accuracy at epoch 48: 0.98\n",
      "step 48 successfully save dir : ./checkpoint/test_model-48\n",
      "===EPOCH   49===\n",
      "[Test]Accuracy at epoch 49: 0.822\n",
      "[Training]Accuracy at epoch 49: 0.99\n",
      "step 49 successfully save dir : ./checkpoint/test_model-49\n",
      "===EPOCH   50===\n",
      "[Test]Accuracy at epoch 50: 0.349\n",
      "[Training]Accuracy at epoch 50: 0.34\n",
      "step 50 successfully save dir : ./checkpoint/test_model-50\n",
      "===EPOCH   51===\n",
      "[Test]Accuracy at epoch 51: 0.59\n",
      "[Training]Accuracy at epoch 51: 0.62\n",
      "step 51 successfully save dir : ./checkpoint/test_model-51\n",
      "===EPOCH   52===\n",
      "[Test]Accuracy at epoch 52: 0.7\n",
      "[Training]Accuracy at epoch 52: 0.76\n",
      "step 52 successfully save dir : ./checkpoint/test_model-52\n",
      "===EPOCH   53===\n",
      "[Test]Accuracy at epoch 53: 0.767\n",
      "[Training]Accuracy at epoch 53: 0.83\n",
      "step 53 successfully save dir : ./checkpoint/test_model-53\n",
      "===EPOCH   54===\n",
      "[Test]Accuracy at epoch 54: 0.816\n",
      "[Training]Accuracy at epoch 54: 0.91\n",
      "step 54 successfully save dir : ./checkpoint/test_model-54\n",
      "===EPOCH   55===\n",
      "[Test]Accuracy at epoch 55: 0.81\n",
      "[Training]Accuracy at epoch 55: 0.96\n",
      "step 55 successfully save dir : ./checkpoint/test_model-55\n",
      "===EPOCH   56===\n",
      "[Test]Accuracy at epoch 56: 0.836\n",
      "[Training]Accuracy at epoch 56: 0.97\n",
      "step 56 successfully save dir : ./checkpoint/test_model-56\n",
      "===EPOCH   57===\n",
      "[Test]Accuracy at epoch 57: 0.821\n",
      "[Training]Accuracy at epoch 57: 0.98\n",
      "step 57 successfully save dir : ./checkpoint/test_model-57\n",
      "===EPOCH   58===\n",
      "[Test]Accuracy at epoch 58: 0.817\n",
      "[Training]Accuracy at epoch 58: 0.98\n",
      "step 58 successfully save dir : ./checkpoint/test_model-58\n",
      "===EPOCH   59===\n",
      "[Test]Accuracy at epoch 59: 0.829\n",
      "[Training]Accuracy at epoch 59: 1.0\n",
      "step 59 successfully save dir : ./checkpoint/test_model-59\n",
      "===EPOCH   60===\n",
      "[Test]Accuracy at epoch 60: 0.843\n",
      "[Training]Accuracy at epoch 60: 0.99\n",
      "step 60 successfully save dir : ./checkpoint/test_model-60\n",
      "===EPOCH   61===\n",
      "[Test]Accuracy at epoch 61: 0.836\n",
      "[Training]Accuracy at epoch 61: 0.99\n",
      "step 61 successfully save dir : ./checkpoint/test_model-61\n",
      "===EPOCH   62===\n",
      "[Test]Accuracy at epoch 62: 0.553\n",
      "[Training]Accuracy at epoch 62: 0.46\n",
      "step 62 successfully save dir : ./checkpoint/test_model-62\n",
      "===EPOCH   63===\n",
      "[Test]Accuracy at epoch 63: 0.838\n",
      "[Training]Accuracy at epoch 63: 0.91\n",
      "step 63 successfully save dir : ./checkpoint/test_model-63\n",
      "===EPOCH   64===\n",
      "[Test]Accuracy at epoch 64: 0.835\n",
      "[Training]Accuracy at epoch 64: 1.0\n",
      "step 64 successfully save dir : ./checkpoint/test_model-64\n",
      "===EPOCH   65===\n",
      "[Test]Accuracy at epoch 65: 0.839\n",
      "[Training]Accuracy at epoch 65: 1.0\n",
      "step 65 successfully save dir : ./checkpoint/test_model-65\n",
      "===EPOCH   66===\n",
      "[Test]Accuracy at epoch 66: 0.837\n",
      "[Training]Accuracy at epoch 66: 1.0\n",
      "step 66 successfully save dir : ./checkpoint/test_model-66\n",
      "===EPOCH   67===\n",
      "[Test]Accuracy at epoch 67: 0.845\n",
      "[Training]Accuracy at epoch 67: 1.0\n",
      "step 67 successfully save dir : ./checkpoint/test_model-67\n",
      "===EPOCH   68===\n",
      "[Test]Accuracy at epoch 68: 0.856\n",
      "[Training]Accuracy at epoch 68: 1.0\n",
      "step 68 successfully save dir : ./checkpoint/test_model-68\n",
      "===EPOCH   69===\n",
      "[Test]Accuracy at epoch 69: 0.847\n",
      "[Training]Accuracy at epoch 69: 0.99\n",
      "step 69 successfully save dir : ./checkpoint/test_model-69\n",
      "===EPOCH   70===\n",
      "[Test]Accuracy at epoch 70: 0.868\n",
      "[Training]Accuracy at epoch 70: 1.0\n",
      "step 70 successfully save dir : ./checkpoint/test_model-70\n",
      "===EPOCH   71===\n",
      "[Test]Accuracy at epoch 71: 0.857\n",
      "[Training]Accuracy at epoch 71: 0.99\n",
      "step 71 successfully save dir : ./checkpoint/test_model-71\n",
      "===EPOCH   72===\n",
      "[Test]Accuracy at epoch 72: 0.845\n",
      "[Training]Accuracy at epoch 72: 1.0\n",
      "step 72 successfully save dir : ./checkpoint/test_model-72\n",
      "===EPOCH   73===\n",
      "[Test]Accuracy at epoch 73: 0.849\n",
      "[Training]Accuracy at epoch 73: 0.98\n",
      "step 73 successfully save dir : ./checkpoint/test_model-73\n",
      "===EPOCH   74===\n",
      "[Test]Accuracy at epoch 74: 0.842\n",
      "[Training]Accuracy at epoch 74: 0.97\n",
      "step 74 successfully save dir : ./checkpoint/test_model-74\n",
      "===EPOCH   75===\n",
      "[Test]Accuracy at epoch 75: 0.855\n",
      "[Training]Accuracy at epoch 75: 0.99\n",
      "step 75 successfully save dir : ./checkpoint/test_model-75\n",
      "===EPOCH   76===\n",
      "[Test]Accuracy at epoch 76: 0.844\n",
      "[Training]Accuracy at epoch 76: 0.99\n",
      "step 76 successfully save dir : ./checkpoint/test_model-76\n",
      "===EPOCH   77===\n",
      "[Test]Accuracy at epoch 77: 0.847\n",
      "[Training]Accuracy at epoch 77: 0.98\n",
      "step 77 successfully save dir : ./checkpoint/test_model-77\n",
      "===EPOCH   78===\n",
      "[Test]Accuracy at epoch 78: 0.362\n",
      "[Training]Accuracy at epoch 78: 0.3\n",
      "step 78 successfully save dir : ./checkpoint/test_model-78\n",
      "===EPOCH   79===\n",
      "[Test]Accuracy at epoch 79: 0.53\n",
      "[Training]Accuracy at epoch 79: 0.45\n",
      "step 79 successfully save dir : ./checkpoint/test_model-79\n",
      "===EPOCH   80===\n",
      "[Test]Accuracy at epoch 80: 0.655\n",
      "[Training]Accuracy at epoch 80: 0.64\n",
      "step 80 successfully save dir : ./checkpoint/test_model-80\n",
      "===EPOCH   81===\n",
      "[Test]Accuracy at epoch 81: 0.764\n",
      "[Training]Accuracy at epoch 81: 0.79\n",
      "step 81 successfully save dir : ./checkpoint/test_model-81\n",
      "===EPOCH   82===\n",
      "[Test]Accuracy at epoch 82: 0.797\n",
      "[Training]Accuracy at epoch 82: 0.84\n",
      "step 82 successfully save dir : ./checkpoint/test_model-82\n",
      "===EPOCH   83===\n",
      "[Test]Accuracy at epoch 83: 0.826\n",
      "[Training]Accuracy at epoch 83: 0.92\n",
      "step 83 successfully save dir : ./checkpoint/test_model-83\n",
      "===EPOCH   84===\n",
      "[Test]Accuracy at epoch 84: 0.822\n",
      "[Training]Accuracy at epoch 84: 0.97\n",
      "step 84 successfully save dir : ./checkpoint/test_model-84\n",
      "===EPOCH   85===\n",
      "[Test]Accuracy at epoch 85: 0.84\n",
      "[Training]Accuracy at epoch 85: 0.99\n",
      "step 85 successfully save dir : ./checkpoint/test_model-85\n"
     ]
    }
   ],
   "source": [
    "X= tf.placeholder(tf.float32,shape=[None,32,32,3])   #input\n",
    "Y_label = tf.placeholder(tf.float32,shape=[None,10])  #Y_label\n",
    "\n",
    "CHECK_POINT_DIR = './checkpoint' #saver를 저장할 장소\n",
    "    \n",
    "class_names = load_class_names()  #class name을 불러옴\n",
    "print(class_names)  #['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "vgg= VGG19(None,None)\n",
    "logits, _=vgg.build_model(X, False)   #vgg model build\n",
    "\n",
    "cost = vgg.inference_loss(out=logits, t=Y_label) #cost function\n",
    "train =trainOptimizer(cost = cost) #adamOptimizer를 불러옴\n",
    "\n",
    "prediction = tf.equal(tf.argmax(logits, 1) , tf.argmax(Y_label, 1))  #prediction\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))  #정확도 계산\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    last_epoch = tf.Variable(0, name='last_epoch')   #마지막에 돌았던 epoch 저장\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = restoreSaver(session = sess,       #이전에 돌았던 학습이 있으면 불러옴\n",
    "                               check_dir = CHECK_POINT_DIR)\n",
    "    start_from = sess.run(last_epoch)\n",
    "    \n",
    "    print(\"Learinig Start....\")\n",
    "    images_train, cls_train, labels_train = load_training_data()   #training_data load\n",
    "    images_test, cls_test, labels_test = load_test_data()          #test_data load\n",
    "    \n",
    "    batch_size =100 \n",
    "    batch_start_idx =0          #batch 시작 인덱스\n",
    "    batch_end_idx = batch_size  #batch 끝 인덱스\n",
    "    maxidx = images_train.shape[0]\n",
    "    \n",
    "    total_batch = int(maxidx/batch_size)  #한 epoch을 돌기 위해 돌아야 하는 횟수\n",
    "    epoch_run=1000\n",
    "    print(\"트레이닝 데이터 수 : \",maxidx)  \n",
    "\n",
    "    for epoch in range(start_from, epoch_run):       #학습시작\n",
    "        for step in range(total_batch):\n",
    "            trainingData = images_train[batch_start_idx:batch_end_idx]     #trainImage[start:end]\n",
    "            Y = labels_train[batch_start_idx:batch_end_idx]                #label[start:end]\n",
    "            sess.run(train,feed_dict={X:trainingData, \n",
    "                                      Y_label:Y})\n",
    "            batch_start_idx = (batch_start_idx + batch_size) % maxidx     #한 step 돌면 index에 batch_size를 더해줌\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            \n",
    "        import random\n",
    "        random_batch = random.randint(0, len(images_test)-1000)       #test data를 무작위로 받아와서 test함\n",
    "        acc = sess.run(accuracy,feed_dict={X:images_test[random_batch:random_batch+1000],\n",
    "                                               Y_label:labels_test[random_batch:random_batch+1000]})\n",
    "           \n",
    "        print('===EPOCH %4s===' % (epoch))\n",
    "        print(\"[Test]Accuracy at epoch %s: %s\" % (epoch, acc))\n",
    "            \n",
    "        acc = sess.run(accuracy,feed_dict={X: images_train[batch_start_idx:batch_end_idx], #training acuuracy\n",
    "                                               Y_label:labels_train[batch_start_idx:batch_end_idx]})\n",
    "        \n",
    "        print(\"[Training]Accuracy at epoch %s: %s\" % (epoch, acc))\n",
    "\n",
    "        saver = saveSaver(session=sess,          #한 epoch 돌 때마다 saver에 여태까지 학습했던 결과를 저장\n",
    "                          last_epoch=last_epoch,\n",
    "                          global_step=epoch,\n",
    "                          saver=saver,\n",
    "                          check_dir=CHECK_POINT_DIR,\n",
    "                          model_name='test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
